# Порівняння Docker-образів TorchScript inference

## Розміри образів
- ml-fat: 1.5 GB
- ml-slim: 1.04 GB

## Кількість шарів
- ml-fat: 14 шарів (зайві build-essential, apt-cache, wget тощо)
- ml-slim: 13 шарів (менше, немає зайвих системних залежностей на фінальному етапі)

## Висновки
- Обидва образи успішно виконують inference на TorchScript-моделі.
- Slim-образ менший на ~500 MB завдяки multi-stage підходу та видаленню непотрібних build-інструментів.

## Подальша оптимізація
- Використати ще легший базовий образ (наприклад, distroless).
- Видалити torchvision, якщо використовуються тільки torch + власний препроцес.
- Використати quantization або ONNX Runtime для зменшення ваги бібліотек.
- Заморозити залежності в `requirements.txt` для відтворюваності.
